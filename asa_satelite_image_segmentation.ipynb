{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(width, height):\n",
    "    transform = A.Compose([\n",
    "\n",
    "        A.RandomCrop(width=width, height=height, p=1.0),\n",
    "        A.HorizontalFlip(p=1.0),\n",
    "        A.VerticalFlip(p=1.0),\n",
    "        A.Rotate(limit=[60, 300], p=1.0, interpolation=cv2.INTER_NEAREST),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[-0.2, 0.3], contrast_limit=0.2, p=1.0),\n",
    "\n",
    "        A.OneOf([\n",
    "            A.CLAHE (clip_limit=1.5, tile_grid_size=(8, 8), p=0.5),                    # Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n",
    "            A.GridDistortion(p=0.5),                                                   # Apply Contrast Limited Adaptive Histogram Equalization to the input image.\n",
    "            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, interpolation=cv2.INTER_NEAREST, p=0.5),\n",
    "        ], p=1.0),\n",
    "    ], p=1.0)\n",
    "    \n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    if original_image is None and original_image is None :\n",
    "        f, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
    "        f.set_tight_layout(h_pad=5, w_pad=5)\n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else :\n",
    "        f, ax = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        plt.tight_layout(pad=0.2, w_pad=1.0, h_pad=0.01)\n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original Image', fontsize=fontsize)\n",
    "\n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original Mask', fontsize=fontsize)\n",
    "\n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed Image', fontsize=fontsize)\n",
    "\n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed Mask', fontsize=fontsize)\n",
    "\n",
    "    plt.savefig('sample_augmented_image.png', facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_t1_001' 'image_t1_002' 'image_t1_003' 'image_t1_004'\n",
      " 'image_t1_005' 'image_t1_006' 'image_t1_007' 'image_t1_008'\n",
      " 'image_t1_009' 'image_t2_001' 'image_t2_002' 'image_t2_003'\n",
      " 'image_t2_004' 'image_t2_005' 'image_t2_006' 'image_t2_007'\n",
      " 'image_t2_008' 'image_t2_009' 'image_t3_001' 'image_t3_002'\n",
      " 'image_t3_003' 'image_t3_004' 'image_t3_005' 'image_t3_006'\n",
      " 'image_t3_007' 'image_t3_008' 'image_t3_009' 'image_t4_001'\n",
      " 'image_t4_002' 'image_t4_003' 'image_t4_004' 'image_t4_005'\n",
      " 'image_t4_006' 'image_t4_007' 'image_t4_008' 'image_t4_009'\n",
      " 'image_t5_001' 'image_t5_002' 'image_t5_003' 'image_t5_004'\n",
      " 'image_t5_005' 'image_t5_006' 'image_t5_007' 'image_t5_008'\n",
      " 'image_t5_009' 'image_t6_001' 'image_t6_002' 'image_t6_003'\n",
      " 'image_t6_004' 'image_t6_005' 'image_t6_006' 'image_t6_007'\n",
      " 'image_t6_008' 'image_t6_009' 'image_t7_001' 'image_t7_002'\n",
      " 'image_t7_003' 'image_t7_004' 'image_t7_005' 'image_t7_006'\n",
      " 'image_t7_007' 'image_t7_008' 'image_t7_009' 'image_t8_001'\n",
      " 'image_t8_002' 'image_t8_003' 'image_t8_004' 'image_t8_005'\n",
      " 'image_t8_006' 'image_t8_007' 'image_t8_008' 'image_t8_009']\n"
     ]
    }
   ],
   "source": [
    "images_dir = \"Semantic_segmentation_dataset/train/train_images/\"\n",
    "masks_dir = \"Semantic_segmentation_dataset/train/train_masks/\"\n",
    "file_names = np.sort(os.listdir(images_dir)) \n",
    "file_names = np.char.split(file_names, '.')\n",
    "filenames = np.array([])\n",
    "for i in range(len(file_names)):\n",
    "    filenames = np.append(filenames, file_names[i][0])\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(count):\n",
    "    \n",
    "    transform_1 = augment(512, 512)\n",
    "    transform_2 = augment(480, 480)\n",
    "    transform_3 = augment(512, 512)\n",
    "    transform_4 = augment(800, 800)\n",
    "    transform_5 = augment(1024, 1024)\n",
    "    transform_6 = augment(800, 800)\n",
    "    transform_7 = augment(1600, 1600)\n",
    "    transform_8 = augment(1920, 1280)\n",
    "    \n",
    "    for i in range(count):\n",
    "        for file in filenames:\n",
    "            tile = file.split('_')[1]\n",
    "            img = cv2.imread(images_dir+file+'.jpg')\n",
    "            \n",
    "            mask = cv2.imread(masks_dir+file+'.png')\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if tile == 't1':\n",
    "                transformed = transform_1(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t2':\n",
    "                transformed = transform_2(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t3':\n",
    "                transformed = transform_3(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t4':\n",
    "                transformed = transform_4(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t5':\n",
    "                transformed = transform_5(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t6':\n",
    "                transformed = transform_6(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t7':\n",
    "                transformed = transform_7(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "            elif tile =='t8':\n",
    "                transformed = transform_8(image=img, mask=mask)\n",
    "                transformed_image = transformed['image']\n",
    "                transformed_mask = transformed['mask']\n",
    "                \n",
    "            cv2.imwrite('Semantic_segmentation_dataset/train/aug_images/aug_{}_'.format(str(i+1))+file+'.jpg',cv2.cvtColor(transformed_image, cv2.COLOR_BGR2RGB))\n",
    "            cv2.imwrite('Semantic_segmentation_dataset/train/aug_masks/aug_{}_'.format(str(i+1))+file+'.png',cv2.cvtColor(transformed_mask, cv2.COLOR_BGR2RGB))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_dataset(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>r</th>\n",
       "      <th>g</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>building</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>land</td>\n",
       "      <td>132</td>\n",
       "      <td>41</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>road</td>\n",
       "      <td>110</td>\n",
       "      <td>193</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vegetation</td>\n",
       "      <td>254</td>\n",
       "      <td>221</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>water</td>\n",
       "      <td>226</td>\n",
       "      <td>169</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unlabeled</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    r    g    b\n",
       "0    building   60   16  152\n",
       "1        land  132   41  246\n",
       "2        road  110  193  228\n",
       "3  vegetation  254  221   58\n",
       "4       water  226  169   41\n",
       "5   unlabeled  155  155  155"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = pd.read_csv(\"class_dict.csv\", index_col=False, skipinitialspace=True)\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(60, 16, 152),\n",
       "  (132, 41, 246),\n",
       "  (110, 193, 228),\n",
       "  (254, 221, 58),\n",
       "  (226, 169, 41),\n",
       "  (155, 155, 155)],\n",
       " ['building', 'land', 'road', 'vegetation', 'water', 'unlabeled'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names= list(class_dict.name)\n",
    "label_codes = []\n",
    "r= np.asarray(class_dict['r'])\n",
    "g= np.asarray(class_dict['g'])\n",
    "b= np.asarray(class_dict['b'])\n",
    "\n",
    "for i in range(len(class_dict)):\n",
    "    label_codes.append(tuple([r[i], g[i], b[i]]))\n",
    "    \n",
    "label_codes, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2id = {v:k for k,v in enumerate(label_codes)}\n",
    "id2code = {k:v for k,v in enumerate(label_codes)}\n",
    "\n",
    "name2id = {v:k for k,v in enumerate(label_names)}\n",
    "id2name = {k:v for k,v in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (60, 16, 152),\n",
       " 1: (132, 41, 246),\n",
       " 2: (110, 193, 228),\n",
       " 3: (254, 221, 58),\n",
       " 4: (226, 169, 41),\n",
       " 5: (155, 155, 155)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'building',\n",
       " 1: 'land',\n",
       " 2: 'road',\n",
       " 3: 'vegetation',\n",
       " 4: 'water',\n",
       " 5: 'unlabeled'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_onehot(rgb_image, colormap = id2code):\n",
    "    num_classes = len(colormap)\n",
    "    shape = rgb_image.shape[:2]+(num_classes,)\n",
    "    encoded_image = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(colormap):\n",
    "        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n",
    "    return encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_rgb(onehot, colormap = id2code):\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in colormap.keys():\n",
    "        output[single_layer==k] = colormap[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rescale=1./255)\n",
    "mask_gen_args = dict()\n",
    "\n",
    "train_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "train_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "val_frames_datagen = ImageDataGenerator(**data_gen_args)\n",
    "val_masks_datagen = ImageDataGenerator(**mask_gen_args)\n",
    "\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = \"Semantic_segmentation_dataset/train/aug_images\"\n",
    "val_images = \"Semantic_segmentation_dataset/train/train_images\"\n",
    "\n",
    "batch_size = 16\n",
    "num_train_samples = len(np.sort(os.listdir(train_images)))\n",
    "num_val_samples = len(np.sort(os.listdir(val_images)))\n",
    "steps_per_epoch = np.ceil(float(num_train_samples) / float(batch_size))\n",
    "print('steps_per_epoch: ', steps_per_epoch)\n",
    "validation_steps = np.ceil(float(4 * num_val_samples) / float(batch_size))\n",
    "print('validation_steps: ', validation_steps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "042f1467ab0111aa983a1a2722ed4e9691068ebac51e10c18c39c936081e164b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
